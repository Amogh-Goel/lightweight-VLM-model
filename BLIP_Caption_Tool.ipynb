{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "658a8e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D(nf=2304, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=768)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (crossattention): GPT2Attention(\n",
       "            (c_attn): Conv1D(nf=1536, nx=768)\n",
       "            (q_attn): Conv1D(nf=768, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=768)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D(nf=3072, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=3072)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcfe4135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image paths: ['C:/Users/admin/Documents/wordpress/wp-content/themes/twentytwentytwo/assets/images/flight-path-on-transparent-b.png']\n",
      "Displaying image: C:/Users/admin/Documents/wordpress/wp-content/themes/twentytwentytwo/assets/images/flight-path-on-transparent-b.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\vlm_env\\lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption with BLIP.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image as PILImage, ImageTk\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load BLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "# Initialize GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"BLIP Image Captioning Tool\")\n",
    "root.geometry(\"800x700\")  # Medium-sized window\n",
    "\n",
    "# Global variables\n",
    "image_paths = []\n",
    "captions = {}\n",
    "current_index = 0\n",
    "csv_path = \"captions.csv\"\n",
    "\n",
    "# GUI elements\n",
    "img_label = tk.Label(root)\n",
    "img_label.pack(pady=10)\n",
    "\n",
    "caption_label = tk.Label(root, text=\"\", wraplength=700, font=(\"Arial\", 12))\n",
    "caption_label.pack(pady=10)\n",
    "\n",
    "correction_entry = tk.Entry(root, width=80)\n",
    "confirm_button = tk.Button(root, text=\"Confirm Correction\", command=lambda: confirm_correction())\n",
    "correction_entry.pack_forget()\n",
    "confirm_button.pack_forget()\n",
    "\n",
    "# Functions\n",
    "def generate_caption(image_path):\n",
    "    raw_image = PILImage.open(image_path).convert('RGB').resize((300, 300))\n",
    "    inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "def load_images():\n",
    "    global image_paths, current_index, captions\n",
    "    files = filedialog.askopenfilenames(filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png\")])\n",
    "    if not files:\n",
    "        caption_label.config(text=\"‚ö†Ô∏è No images selected.\")\n",
    "        return\n",
    "\n",
    "    image_paths = list(files)\n",
    "    current_index = 0\n",
    "    captions = {}\n",
    "    print(\"Loaded image paths:\", image_paths)  # Debug\n",
    "    load_captions_from_csv()\n",
    "    show_image_at_index(current_index)\n",
    "\n",
    "def show_image_at_index(index):\n",
    "    correction_entry.pack_forget()\n",
    "    confirm_button.pack_forget()\n",
    "\n",
    "    if index < 0 or index >= len(image_paths):\n",
    "        caption_label.config(text=\"‚ö†Ô∏è No more images.\")\n",
    "        img_label.config(image='')\n",
    "        img_label.image = None\n",
    "        return\n",
    "\n",
    "    path = image_paths[index]\n",
    "    print(\"Displaying image:\", path)  # Debug\n",
    "\n",
    "    try:\n",
    "        img = PILImage.open(path).resize((400, 400))\n",
    "        tk_img = ImageTk.PhotoImage(img)\n",
    "        img_label.config(image=tk_img)\n",
    "        img_label.image = tk_img  # Prevent garbage collection\n",
    "    except Exception as e:\n",
    "        caption_label.config(text=f\"‚ö†Ô∏è Failed to load image: {e}\")\n",
    "        return\n",
    "\n",
    "    if path in captions and captions[path].strip() != \"\":\n",
    "        caption = captions[path]\n",
    "    else:\n",
    "        try:\n",
    "            caption = generate_caption(path)\n",
    "            captions[path] = caption  # Save it for later use\n",
    "            print(\"Generated caption with BLIP.\")\n",
    "        except Exception as e:\n",
    "            caption = \"‚ö†Ô∏è Captioning failed\"\n",
    "            print(\"Captioning error:\", e)\n",
    "\n",
    "    caption_label.config(text=f\"Caption: {caption}\")\n",
    "        \n",
    "\n",
    "def show_next_image():\n",
    "    global current_index\n",
    "\n",
    "    if current_index < len(image_paths) - 1:\n",
    "        current_index += 1\n",
    "        show_image_at_index(current_index)\n",
    "    else:\n",
    "        # Final image already shown, now wrap up\n",
    "        caption_label.config(text=\"‚úÖ All images have been processed.\")\n",
    "        image_label.config(image='')  # Optional: clear image display\n",
    "        next_button.config(state='disabled')\n",
    "        correction_entry.config(state='disabled')\n",
    "        save_button.config(state='disabled')\n",
    "\n",
    "        # Optional: show popup\n",
    "        from tkinter import messagebox\n",
    "        messagebox.showinfo(\"Done\", \"üéâ All images have been captioned and saved.\")\n",
    "def show_previous_image():\n",
    "    global current_index\n",
    "    if current_index > 0:\n",
    "        current_index -= 1\n",
    "        show_image_at_index(current_index)\n",
    "\n",
    "def show_correction_field():\n",
    "    path = image_paths[current_index]\n",
    "    correction_entry.delete(0, tk.END)\n",
    "    correction_entry.insert(0, captions[path])\n",
    "    correction_entry.pack()\n",
    "    confirm_button.pack()\n",
    "\n",
    "def confirm_correction():\n",
    "    path = image_paths[current_index]\n",
    "    corrected = correction_entry.get()\n",
    "    captions[path] = corrected\n",
    "    caption_label.config(text=f\"Corrected Caption: {corrected}\")\n",
    "    correction_entry.pack_forget()\n",
    "    confirm_button.pack_forget()\n",
    "    save_captions_to_csv()\n",
    "\n",
    "def save_captions_to_csv():\n",
    "    with open(csv_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Image\", \"Caption\"])\n",
    "        for path, caption in captions.items():\n",
    "            writer.writerow([path, caption])\n",
    "\n",
    "def load_captions_from_csv():\n",
    "    if not os.path.exists(csv_path):\n",
    "        return\n",
    "    with open(csv_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            captions[row[\"Image\"]] = row[\"Caption\"]\n",
    "\n",
    "def export_to_csv():\n",
    "    export_path = filedialog.asksaveasfilename(defaultextension=\".csv\")\n",
    "    with open(export_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Image Path\", \"Caption\"])\n",
    "        for path in image_paths:\n",
    "            final_caption = captions.get(path, \"\")\n",
    "            writer.writerow([path, final_caption])\n",
    "    caption_label.config(text=\"‚úÖ Export complete!\")\n",
    "\n",
    "# Buttons\n",
    "load_button = tk.Button(root, text=\"üñºÔ∏è Add Image(s)\", command=load_images)\n",
    "load_button.pack(pady=5)\n",
    "\n",
    "correct_button = tk.Button(root, text=\"‚úèÔ∏è Correct Caption\", command=show_correction_field)\n",
    "correct_button.pack(pady=5)\n",
    "\n",
    "back_button = tk.Button(root, text=\"‚¨ÖÔ∏è Back\", command=show_previous_image)\n",
    "back_button.pack(pady=5)\n",
    "\n",
    "next_button = tk.Button(root, text=\"‚û°Ô∏è Next\", command=show_next_image)\n",
    "next_button.pack(pady=5)\n",
    "\n",
    "export_button = tk.Button(root, text=\"üíæ Export Captions\", command=export_to_csv)\n",
    "export_button.pack(pady=10)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3dbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vlm_env)",
   "language": "python",
   "name": "vlm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
